{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://www.kaggle.com/c/bike-sharing-demand\\ndatetime - hourly date + timestamp  \\nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \\nholiday - whether the day is considered a holiday\\nworkingday - whether the day is neither a weekend nor holiday\\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \\ntemp - temperature in Celsius\\natemp - \"feels like\" temperature in Celsius\\nhumidity - relative humidity\\nwindspeed - wind speed\\ncasual - number of non-registered user rentals initiated\\nregistered - number of registered user rentals initiated\\ncount - number of total rentals\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregar base de dados\n",
    "from test_helper import Test\n",
    "import os.path\n",
    "baseDir = os.path.join('Data')\n",
    "inputPath = os.path.join('Aula04', 'BikeSharing.csv')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "numPartitions = 2\n",
    "csvData = sc.textFile(fileName, numPartitions)\n",
    "header = csvData.take(1)[0]\n",
    "rawData = csvData.filter(lambda x: x!=header)\n",
    "\n",
    "'''\n",
    "https://www.kaggle.com/c/bike-sharing-demand\n",
    "datetime - hourly date + timestamp  \n",
    "season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "holiday - whether the day is considered a holiday\n",
    "workingday - whether the day is neither a weekend nor holiday\n",
    "weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "temp - temperature in Celsius\n",
    "atemp - \"feels like\" temperature in Celsius\n",
    "humidity - relative humidity\n",
    "windspeed - wind speed\n",
    "casual - number of non-registered user rentals initiated\n",
    "registered - number of registered user rentals initiated\n",
    "count - number of total rentals\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def discrete(value, max, n):\n",
    "    \"\"\"Divide o intervalo de 0 até 'max' em 'n' intervalos de mesmo tamanho e retorna qual intervalo 'value' corresponde.\n",
    "    \n",
    "    Args:\n",
    "        value: valor a ser discretizado.\n",
    "        max: valor maximo que 'value' pode assumir.\n",
    "        n: número de classes que serão levadas\n",
    "    \"\"\"\n",
    "    \n",
    "    if value==max: value-=1\n",
    "    gap=max/n\n",
    "    return int(math.floor(value/gap))\n",
    "\n",
    "    \n",
    "\n",
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    data = point.split(',')\n",
    "    Date = dt.strptime(data[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "    DateList = [Date.year-2011, Date.month, Date.hour]   \n",
    "    season = data[1]\n",
    "    workingday=data[3]\n",
    "    weather =data[4]\n",
    "    sense=discrete(float(data[6]),46.,8)\n",
    "    hum=discrete(float(data[7]),100.,8)\n",
    "    wind=discrete(float(data[8]),57.,8)\n",
    "    #features = DenseVector(DateList + season + realValues)\n",
    "    features = DenseVector(DateList + [season]+ [workingday] + [sense] + [hum] + [wind] + [weather])# + realValues)\n",
    "    \n",
    "    label = discrete(int(data[-1]),997,10)\n",
    "    \n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "\n",
    "rawTrainData, rawValData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "\n",
    "parsedTrainData = rawTrainData.map(parsePoint).cache()\n",
    "parsedValData = rawValData.map(parsePoint).cache()\n",
    "parsedTestData = rawTestData.map(parsePoint).cache()\n",
    "\n",
    "print parsedTrainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Função própria do Spark\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(parsedTrainData, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = parsedTestData.map(lambda p : (model.predict(p.features), p.label))\n",
    "a=parsedTrainData.filter(lambda lp:lp.label==9)\n",
    "print a.take(1)[0].label\n",
    "model.predict(a.take(1)[0].features)\n",
    "#accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / parsedTestData.count()\n",
    "#print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def naiveBayesTrain(trainSet):\n",
    "    \"\"\"Recebe um set de treinos e calcula probabilidades e cria um modelo\n",
    "\n",
    "    Args:\n",
    "        trainSet (list)->(label, features)\n",
    "\n",
    "    Returns:\n",
    "        list: Um model, array[\n",
    "                                Dict class : probabilidade(class),\n",
    "                                RDD de tuplas (atributo, probabilidade(atributo))\n",
    "                                RDD de tuplas ((class, atributo), probabilidade(atributo | class))\n",
    "                                Int Quantidade de classes\n",
    "                             ]\n",
    "    \"\"\"\n",
    "    #Tiramos a quantidade de atributos de uma amostra\n",
    "    amostra = trainSet.take(1)[0]\n",
    "    #quantidade de atributos\n",
    "    nAttri=len(amostra.features)\n",
    "    \n",
    "    RDDlen=trainSet.count()\n",
    "    print RDDlen\n",
    "    #Probabilidade de cada classe\n",
    "    #Probabilidade de cada classe, sem logartimo, para uso na probabilidade condicional \n",
    "    #MODIFICADO: reaproveitamento de PClassesWithoutlog\n",
    "    nClasses=(trainSet\n",
    "                            .map(lambda lp:(lp.label,1))\n",
    "                            .reduceByKey(lambda x,y:x+y))\n",
    "    PClassesWithoutlog=(nClasses\n",
    "                            .map(lambda (x,y):(x,(y+1)/float(RDDlen+nAttri))))\n",
    "    PClassesRDD = (PClassesWithoutlog\n",
    "                        .map(lambda (x,y):(x,np.log10(y))))\n",
    "    #Podemos dar colect pra virar um dict, e facilitar o acesso, numero de classes nao sera grande suficiente para estourar\n",
    "    \n",
    "    nClasses=nClasses.collectAsMap()\n",
    "    PClassesWithoutlog = PClassesWithoutlog.collectAsMap()\n",
    "    PClassesRDD = PClassesRDD.collectAsMap()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Probabilidade de cada atributo\n",
    "    #lista de RDDs para cada atributo\n",
    "    AttributesRDDs=[]\n",
    "    #lista de RDDs para probabilidade de cada atributo dado classe\n",
    "    AttributeClassesRDDs=[]\n",
    "    \n",
    "    #laço para percorrer os atributos\n",
    "    #MODIFICADO: inclusão da normalização de termos nulo\n",
    "    for i in range(nAttri):\n",
    "        AttriRDD =(trainSet\n",
    "                    .map(lambda lp:(lp.features[i],1))\n",
    "                    .reduceByKey(lambda x,y:x+y)\n",
    "                    .map(lambda (x,y):(x,np.log10((y+1)/float(RDDlen+nAttri))))\n",
    "                  )\n",
    "        AttributesRDDs.append(AttriRDD)\n",
    "        AttriClassRDD=(trainSet\n",
    "                        .map(lambda lp:((lp.label,lp.features[i]),1))\n",
    "                        .reduceByKey(lambda x,y:x+y)\n",
    "                        #.map(lambda ((c,a),q) : (x, np.log10((y+1)/float(RDDlen+nAttri))))\n",
    "                        .map(lambda ((c,a),q) : ((c,a), np.log10((q+1)/float(nClasses[c]+nAttri)))))\n",
    "        AttributeClassesRDDs.append(AttriClassRDD)\n",
    "        \n",
    "    return [PClassesRDD, AttributesRDDs, AttributeClassesRDDs, nAttri]\n",
    "\n",
    "def predict(model, objectToPredict):\n",
    "    Pc = model[0]\n",
    "    AttributesRDDs = model[1]\n",
    "    AttributeClassesRDDs = model[2]\n",
    "    nAttri = model[3]\n",
    "    sumClass = -1\n",
    "    bestClass = 0\n",
    "    Pa = []\n",
    "    Pac = []\n",
    "    #Filtrar RDD de probabilidade para conter somente as que sao necessarias para o objeto a ser predito e entao criar um dict\n",
    "    #MODIFICADO\n",
    "    #gera uma lista com as prob. dos atributos\n",
    "    for i in range(nAttri):\n",
    "            Pa.extend((AttributesRDDs[i]\n",
    "                            .filter(lambda (a,p): a == objectToPredict.features[i])\n",
    "                            .map(lambda (a,p):p)).collect())\n",
    "    sumPa=sum(Pa)\n",
    "    for classe in Pc:\n",
    "        Pac=[]\n",
    "        for i in range(nAttri):\n",
    "            #gera uma lista com probabilidades P(classe|objectToPredict.features[i])\n",
    "            Pac.extend((AttributeClassesRDDs[i]\n",
    "                        .filter(lambda ((c,a),p): a == objectToPredict.features[i] and c==classe)\n",
    "                        .map(lambda ((c,a),p):p)).collect())\n",
    "        # Soma segundo formula do slide 87\n",
    "        soma=sum(Pac)+Pc[classe]-sumPa\n",
    "        print str(sum(Pac))+\" \"+str(Pc[classe])+\" \"+str(sumPa)\n",
    "        if(soma > sumClass):\n",
    "            sumClass = soma\n",
    "            bestClass = classe\n",
    "        print \"Classe: \"+str(classe)+\" Soma:\"+str(soma)\n",
    "    return bestClass\n",
    "print \"built!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8761\n",
      "[(0.0, 3440), (8.0, 66), (2.0, 1372), (4.0, 512), (6.0, 176)]\n",
      "[(0.0, 0.39236031927023945), (8.0, 0.007639680729760547), (2.0, 0.15655644241733183), (4.0, 0.058494868871151656), (6.0, 0.020182440136830102)]\n",
      "[(0.0, -0.40631492074511039), (8.0, -2.1169247906652142), (2.0, -0.8053290561292854), (4.0, -1.2328822282542242), (6.0, -1.6950263270042338)]\n",
      "5.0\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'dict'>\n",
      "-7.11540828221 -0.406314920745 -5.9822454894\n",
      "Classe: 0.0 Soma:-1.53947771356\n",
      "-6.1853427832 -0.661966226118 -5.9822454894\n",
      "Classe: 1.0 Soma:-0.865063519927\n",
      "-5.95383905113 -0.805329056129 -5.9822454894\n",
      "Classe: 2.0 Soma:-0.776922617867\n",
      "-5.65864967586 -1.01103347864 -5.9822454894\n",
      "Classe: 3.0 Soma:-0.687437665106\n",
      "-5.59990912075 -1.23288222825 -5.9822454894\n",
      "Classe: 4.0 Soma:-0.850545859613\n",
      "-5.48635603828 -1.43111623239 -5.9822454894\n",
      "Classe: 5.0 Soma:-0.935226781273\n",
      "-6.14807608697 -1.695026327 -5.9822454894\n",
      "Classe: 6.0 Soma:-1.86085692458\n",
      "-4.21540571433 -1.96072836033 -5.9822454894\n",
      "Classe: 7.0 Soma:-0.193888585256\n",
      "-4.71405961611 -2.11692479067 -5.9822454894\n",
      "Classe: 8.0 Soma:-0.848738917384\n",
      "-2.91277285466 -2.79687155769 -5.9822454894\n",
      "Classe: 9.0 Soma:0.27260107705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=naiveBayesTrain(parsedTrainData)\n",
    "#gambiarra para tentar pegar amostra com label=9\n",
    "a=parsedTrainData.filter(lambda lp:lp.label==5)\n",
    "print a.take(1)[0].label\n",
    "predict(lst,a.take(1)[0])\n",
    "#TEM ALGUM ERRO QUE NÃO DEIXA A VALIDAÇÃO DE BAIXO SER CONCLUÍDA\n",
    "#predictionAndLabel = parsedTestData.map(lambda p : (predict(lst,p.features), p.label))\n",
    "#accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / parsedTestData.count()\n",
    "#print accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
