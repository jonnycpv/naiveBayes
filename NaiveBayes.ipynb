{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://www.kaggle.com/c/bike-sharing-demand\\ndatetime - hourly date + timestamp  \\nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \\nholiday - whether the day is considered a holiday\\nworkingday - whether the day is neither a weekend nor holiday\\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \\ntemp - temperature in Celsius\\natemp - \"feels like\" temperature in Celsius\\nhumidity - relative humidity\\nwindspeed - wind speed\\ncasual - number of non-registered user rentals initiated\\nregistered - number of registered user rentals initiated\\ncount - number of total rentals\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregar base de dados\n",
    "from test_helper import Test\n",
    "import os.path\n",
    "baseDir = os.path.join('Data')\n",
    "inputPath = os.path.join('Aula04', 'BikeSharing.csv')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "numPartitions = 2\n",
    "csvData = sc.textFile(fileName, numPartitions)\n",
    "header = csvData.take(1)[0]\n",
    "rawData = csvData.filter(lambda x: x!=header)\n",
    "\n",
    "'''\n",
    "https://www.kaggle.com/c/bike-sharing-demand\n",
    "datetime - hourly date + timestamp  \n",
    "season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "holiday - whether the day is considered a holiday\n",
    "workingday - whether the day is neither a weekend nor holiday\n",
    "weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "temp - temperature in Celsius\n",
    "atemp - \"feels like\" temperature in Celsius\n",
    "humidity - relative humidity\n",
    "windspeed - wind speed\n",
    "casual - number of non-registered user rentals initiated\n",
    "registered - number of registered user rentals initiated\n",
    "count - number of total rentals\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def discrete(value, max, n):\n",
    "    \"\"\"Divide o intervalo de 0 até 'max' em 'n' intervalos de mesmo tamanho e retorna qual intervalo 'value' corresponde.\n",
    "    \n",
    "    Args:\n",
    "        value: valor a ser discretizado.\n",
    "        max: valor maximo que 'value' pode assumir.\n",
    "        n: número de classes que serão levadas\n",
    "    \"\"\"\n",
    "    \n",
    "    if value==max: value-=1\n",
    "    gap=max/n\n",
    "    return int(math.floor(value/gap))\n",
    "\n",
    "    \n",
    "\n",
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    data = point.split(',')\n",
    "    Date = dt.strptime(data[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "    DateList = [Date.year-2011, Date.month, Date.hour]   \n",
    "    season = data[1]\n",
    "    workingday=data[3]\n",
    "    weather =data[4]\n",
    "    sense=discrete(float(data[6]),46.,8)\n",
    "    hum=discrete(float(data[7]),100.,8)\n",
    "    wind=discrete(float(data[8]),57.,8)\n",
    "    #features = DenseVector(DateList + season + realValues)\n",
    "    features = DenseVector(DateList + [season]+ [workingday] + [sense] + [hum] + [wind] + [weather])# + realValues)\n",
    "    \n",
    "    label = discrete(int(data[-1]),997,10)\n",
    "    \n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "\n",
    "rawTrainData, rawValData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "\n",
    "parsedTrainData = rawTrainData.map(parsePoint).cache()\n",
    "parsedValData = rawValData.map(parsePoint).cache()\n",
    "parsedTestData = rawTestData.map(parsePoint).cache()\n",
    "\n",
    "print parsedTrainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Função própria do Spark\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(parsedTrainData, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = parsedTestData.map(lambda p : (model.predict(p.features), p.label))\n",
    "a=parsedTrainData.filter(lambda lp:lp.label==9)\n",
    "print a.take(1)[0].label\n",
    "model.predict(a.take(1)[0].features)\n",
    "#accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / parsedTestData.count()\n",
    "#print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def naiveBayesTrain(trainSet):\n",
    "    \"\"\"Recebe um set de treinos e calcula probabilidades e cria um modelo\n",
    "\n",
    "    Args:\n",
    "        trainSet (list)->(label, features)\n",
    "\n",
    "    Returns:\n",
    "        list: Um model, array[\n",
    "                                Dict class : probabilidade(class),\n",
    "                                RDD de tuplas (atributo, probabilidade(atributo))\n",
    "                                RDD de tuplas ((class, atributo), probabilidade(atributo | class))\n",
    "                                Int Quantidade de classes\n",
    "                             ]\n",
    "    \"\"\"\n",
    "    #Tiramos a quantidade de atributos de uma amostra\n",
    "    amostra = trainSet.take(1)[0]\n",
    "    #quantidade de atributos\n",
    "    nAttri=len(amostra.features)\n",
    "    \n",
    "    RDDlen=trainSet.count()\n",
    "    print RDDlen\n",
    "    #Probabilidade de cada classe\n",
    "    #Probabilidade de cada classe, sem logartimo, para uso na probabilidade condicional \n",
    "    #MODIFICADO: reaproveitamento de PClassesWithoutlog\n",
    "    PClassesWithoutlog=(trainSet\n",
    "                            .map(lambda lp:(lp.label,1))\n",
    "                            .reduceByKey(lambda x,y:x+y)\n",
    "                            .map(lambda (x,y):(x,y+1/float(RDDlen+nAttri))))\n",
    "    PClassesRDD = (PClassesWithoutlog\n",
    "                        .map(lambda (x,y):(x,np.log10(y))))\n",
    "    #Podemos dar colect pra virar um dict, e facilitar o acesso, numero de classes nao sera grande suficiente para estourar\n",
    "    PClassesWithoutlog = PClassesWithoutlog.collectAsMap()\n",
    "    PClassesRDD = PClassesRDD.collectAsMap()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Probabilidade de cada atributo\n",
    "    #lista de RDDs para cada atributo\n",
    "    AttributesRDDs=[]\n",
    "    #lista de RDDs para probabilidade de cada atributo dado classe\n",
    "    AttributeClassesRDDs=[]\n",
    "    \n",
    "    #laço para percorrer os atributos\n",
    "    #MODIFICADO: inclusão da normalização de termos nulo\n",
    "    for i in range(nAttri):\n",
    "        AttriRDD =(trainSet\n",
    "                    .map(lambda lp:(lp.features[i],1))\n",
    "                    .reduceByKey(lambda x,y:x+y)\n",
    "                    .map(lambda (x,y):(x,np.log10((y+1)/float(RDDlen+nAttri))))\n",
    "                  )\n",
    "        AttributesRDDs.append(AttriRDD)\n",
    "        AttriClassRDD=(trainSet\n",
    "                        .map(lambda lp:((lp.label,lp.features[i]),1))\n",
    "                        .reduceByKey(lambda x,y:x+y)\n",
    "                        #.map(lambda ((c,a),q) : (x, np.log10((y+1)/float(RDDlen+nAttri))))\n",
    "                        .map(lambda ((c,a),q) : ((c,a), np.log10(q+1/PClassesWithoutlog[c]+nAttri))) \n",
    "                      )\n",
    "        AttributeClassesRDDs.append(AttriClassRDD)\n",
    "        \n",
    "    return [PClassesRDD, AttributesRDDs, AttributeClassesRDDs, nAttri]\n",
    "\n",
    "def predict(model, objectToPredict):\n",
    "    Pc = model[0]\n",
    "    AttributesRDDs = model[1]\n",
    "    AttributeClassesRDDs = model[2]\n",
    "    nAttri = model[3]\n",
    "    sumClass = -1\n",
    "    bestClass = 0\n",
    "    Pa = []\n",
    "    Pac = []\n",
    "    #Filtrar RDD de probabilidade para conter somente as que sao necessarias para o objeto a ser predito e entao criar um dict\n",
    "    #MODIFICADO\n",
    "    #gera uma lista com as prob. dos atributos\n",
    "    for i in range(nAttri):\n",
    "            Pa.extend((AttributesRDDs[i]\n",
    "                            .filter(lambda (a,p): a == objectToPredict.features[i])\n",
    "                            .map(lambda (a,p):p)).collect())\n",
    "    print Pa\n",
    "    sumPa=sum(Pa)\n",
    "    \n",
    "    for classe in Pc:\n",
    "        Pac=[]\n",
    "        for i in range(nAttri):\n",
    "            #gera uma lista com probabilidades P(classe|objectToPredict.features[i])\n",
    "            Pac.extend((AttributeClassesRDDs[i]\n",
    "                        .filter(lambda ((c,a),p): a == objectToPredict.features[i] and c==classe)\n",
    "                        .map(lambda ((c,a),p):p)).collect())\n",
    "        # Soma segundo formula do slide 87\n",
    "        soma=sum(Pac)+Pc[classe]-sumPa\n",
    "        if(soma > sumClass):\n",
    "            sumClass = soma\n",
    "            bestClass = classe\n",
    "            print bestClass\n",
    "        print \"Classe: \"+str(classe)+\" Soma:\"+str(soma)\n",
    "    return bestClass\n",
    "print \"built!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8761\n",
      "8.0\n",
      "[-0.30043615626165271, -1.0761217790285418, -1.3771517746925228, -0.60834482648279919, -0.1655002737756755, -0.62036688167381704, -0.65296498200352249, -0.5664226363095286, -0.17912256187038564]\n",
      "0.0\n",
      "Classe: 0.0 Soma:34.4953145003\n",
      "Classe: 1.0 Soma:32.6153049002\n",
      "Classe: 2.0 Soma:31.5483837214\n",
      "Classe: 3.0 Soma:29.4906796983\n",
      "Classe: 4.0 Soma:27.5179856588\n",
      "Classe: 5.0 Soma:26.2222023849\n",
      "Classe: 6.0 Soma:23.7881761503\n",
      "Classe: 7.0 Soma:22.1460859487\n",
      "Classe: 8.0 Soma:21.0646867892\n",
      "Classe: 9.0 Soma:15.3091376356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=naiveBayesTrain(parsedTrainData)\n",
    "#gambiarra para tentar pegar amostra com label=9\n",
    "a=parsedTrainData.filter(lambda lp:lp.label==9)\n",
    "print a.take(1)[0].label\n",
    "predict(lst,a.take(1)[0])\n",
    "#TEM ALGUM ERRO QUE NÃO DEIXA A VALIDAÇÃO DE BAIXO SER CONCLUÍDA\n",
    "#predictionAndLabel = parsedTestData.map(lambda p : (predict(lst,p.features), p.label))\n",
    "#accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / parsedTestData.count()\n",
    "#print accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
