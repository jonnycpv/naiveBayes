{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://www.kaggle.com/c/bike-sharing-demand\\ndatetime - hourly date + timestamp  \\nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \\nholiday - whether the day is considered a holiday\\nworkingday - whether the day is neither a weekend nor holiday\\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \\ntemp - temperature in Celsius\\natemp - \"feels like\" temperature in Celsius\\nhumidity - relative humidity\\nwindspeed - wind speed\\ncasual - number of non-registered user rentals initiated\\nregistered - number of registered user rentals initiated\\ncount - number of total rentals\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregar base de dados\n",
    "from test_helper import Test\n",
    "import os.path\n",
    "baseDir = os.path.join('Data')\n",
    "inputPath = os.path.join('Aula04', 'BikeSharing.csv')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "numPartitions = 2\n",
    "csvData = sc.textFile(fileName, numPartitions)\n",
    "header = csvData.take(1)[0]\n",
    "rawData = csvData.filter(lambda x: x!=header)\n",
    "\n",
    "'''\n",
    "https://www.kaggle.com/c/bike-sharing-demand\n",
    "datetime - hourly date + timestamp  \n",
    "season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "holiday - whether the day is considered a holiday\n",
    "workingday - whether the day is neither a weekend nor holiday\n",
    "weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "temp - temperature in Celsius\n",
    "atemp - \"feels like\" temperature in Celsius\n",
    "humidity - relative humidity\n",
    "windspeed - wind speed\n",
    "casual - number of non-registered user rentals initiated\n",
    "registered - number of registered user rentals initiated\n",
    "count - number of total rentals\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def discrete(value, max, n):\n",
    "    \"\"\"Divide o intervalo de 0 até 'max' em 'n' intervalos de mesmo tamanho e retorna qual intervalo 'value' corresponde.\n",
    "    \n",
    "    Args:\n",
    "        value: valor a ser discretizado.\n",
    "        max: valor maximo que 'value' pode assumir.\n",
    "        n: número de classes que serão levadas\n",
    "    \"\"\"\n",
    "    \n",
    "    if value==max: value-=1\n",
    "    gap=max/n\n",
    "    return int(math.floor(value/gap))\n",
    "\n",
    "    \n",
    "\n",
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    data = point.split(',')\n",
    "    Date = dt.strptime(data[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "    DateList = [Date.year-2011, Date.month, Date.hour]   \n",
    "    season = data[1]\n",
    "    workingday=data[3]\n",
    "    weather =data[4]\n",
    "    sense=discrete(float(data[6]),46.,8)\n",
    "    hum=discrete(float(data[7]),100.,8)\n",
    "    wind=discrete(float(data[8]),57.,8)\n",
    "    #features = DenseVector(DateList + season + realValues)\n",
    "    features = DenseVector(DateList + [season]+ [workingday] + [sense] + [hum] + [wind] + [weather])# + realValues)\n",
    "    \n",
    "    label = discrete(int(data[-1]),997,10)\n",
    "    \n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "\n",
    "rawTrainData, rawValData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "\n",
    "parsedTrainData = rawTrainData.map(parsePoint).cache()\n",
    "parsedValData = rawValData.map(parsePoint).cache()\n",
    "parsedTestData = rawTestData.map(parsePoint).cache()\n",
    "\n",
    "print parsedTrainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.426679280984\n"
     ]
    }
   ],
   "source": [
    "#Função própria do Spark\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(parsedTrainData, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = parsedTestData.map(lambda p : (model.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / parsedTestData.count()\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0, 0.0), -0.22927639553818416), ((9.0, 1.0), -2.1668625760868299), ((7.0, 1.0), -1.303082323104819), ((3.0, 1.0), -0.58183592405764806), ((6.0, 0.0), -2.495165757413305), ((2.0, 0.0), -0.77992233432568203), ((5.0, 1.0), -0.88635424756745063), ((1.0, 1.0), -0.39544470836215484), ((4.0, 0.0), -1.2600966383982861), ((0.0, 1.0), -0.13127891463931898)]\n"
     ]
    }
   ],
   "source": [
    "#Fiz os dados do desafio 1 funcionarem com o Bayes, só falta começar agora.\n",
    "import numpy as np\n",
    "\n",
    "#Para cada classe c, calcular P(c). Para cada valor de atributo a, calcular P(a|c)\n",
    "RDDlen=parsedTrainData.count()\n",
    "\n",
    "#Probabilidade de cada classe\n",
    "PClassesRDD=(parsedTrainData\n",
    "                .map(lambda lp:(lp.label,1))\n",
    "                .reduceByKey(lambda x,y:x+y)\n",
    "                .map(lambda (x,y):(x,np.log10(y/float(RDDlen))))\n",
    "            )\n",
    "\n",
    "#Probabilidade de cada classe, sem logartimo, para uso na probabilidade condicional \n",
    "PClassesWithoutlog=(parsedTrainData\n",
    "                .map(lambda lp:(lp.label,1))\n",
    "                .reduceByKey(lambda x,y:x+y)\n",
    "                .map(lambda (x,y):(x,y/float(RDDlen)))\n",
    "            )\n",
    "PClassesWithoutlog = PClassesWithoutlog.collectAsMap()\n",
    "\n",
    "#Probabilidade de cada atributo\n",
    "#Tiramos a quantidade de atributos de uma amostra\n",
    "amostra = parsedTrainData.take(1)[0]\n",
    "\n",
    "#quantidade de atributos\n",
    "nAttri=len(amostra.features)\n",
    "#lista de RDDs para cada atributo\n",
    "AttributesRDDs=[]\n",
    "#lista de RDDs para probabilidade de cada atributo dado classe\n",
    "AttributeClassesRDDs=[]\n",
    "#laço para percorrer os atributos\n",
    "for i in range(nAttri):\n",
    "    AttriRDD =(parsedTrainData\n",
    "                .map(lambda lp:(lp.features[i],1))\n",
    "                .reduceByKey(lambda x,y:x+y)\n",
    "                .map(lambda (x,y):(x,np.log10(y/float(RDDlen)))))\n",
    "    AttributesRDDs.append(AttriRDD)\n",
    "    AttriClassRDD=(parsedTrainData\n",
    "                    .map(lambda lp:((lp.label,lp.features[i]),1))\n",
    "                    .reduceByKey(lambda x,y:x+y)\n",
    "                    .map(lambda (x,y) : (x, y/float(RDDlen)))\n",
    "                    .map(lambda (x,y) : (x, np.log10(y/PClassesWithoutlog[x[1]]))) \n",
    "                  )\n",
    "    AttributeClassesRDDs.append(AttriClassRDD)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def naiveBayesTrain(trainSet):\n",
    "    \"\"\"Recebe um set de treinos e calcula probabilidades e cria um modelo\n",
    "\n",
    "    Args:\n",
    "        trainSet (list)->(label, features)\n",
    "\n",
    "    Returns:\n",
    "        list: Um model, array[\n",
    "                                Dict class : probabilidade(class),\n",
    "                                RDD de tuplas (atributo, probabilidade(atributo))\n",
    "                                RDD de tuplas ((class, atributo), probabilidade(atributo | class))\n",
    "                                Int Quantidade de classes\n",
    "                             ]\n",
    "    \"\"\"\n",
    "    RDDlen=trainSet.count()\n",
    "    #Probabilidade de cada classe\n",
    "    PClassesRDD=(trainSet\n",
    "                .map(lambda lp:(lp.label,1))\n",
    "                .reduceByKey(lambda x,y:x+y)\n",
    "                .map(lambda (x,y):(x,np.log10(y/float(RDDlen))))\n",
    "                )\n",
    "    #Podemos dar colect pra virar um dict, e facilitar o acesso, numero de classes nao sera grande suficiente para estourar\n",
    "    PClassesRDD = PClassesRDD.collectAsMap()\n",
    "    #Probabilidade de cada classe, sem logartimo, para uso na probabilidade condicional \n",
    "    PClassesWithoutlog=(trainSet\n",
    "                                .map(lambda lp:(lp.label,1))\n",
    "                                .reduceByKey(lambda x,y:x+y)\n",
    "                                .map(lambda (x,y):(x,y/float(RDDlen)))\n",
    "                        )\n",
    "    PClassesWithoutlog = PClassesWithoutlog.collectAsMap()\n",
    "    \n",
    "    #Probabilidade de cada atributo\n",
    "    #Tiramos a quantidade de atributos de uma amostra\n",
    "    amostra = trainSet.take(1)[0]\n",
    "\n",
    "    #quantidade de atributos\n",
    "    nAttri=len(amostra.features)\n",
    "    #lista de RDDs para cada atributo\n",
    "    AttributesRDDs=[]\n",
    "    #lista de RDDs para probabilidade de cada atributo dado classe\n",
    "    AttributeClassesRDDs=[]\n",
    "    \n",
    "    #laço para percorrer os atributos\n",
    "    for i in range(nAttri):\n",
    "        AttriRDD =(trainSet\n",
    "                    .map(lambda lp:(lp.features[i],1))\n",
    "                    .reduceByKey(lambda x,y:x+y)\n",
    "                    .map(lambda (x,y):(x,np.log10(y/float(RDDlen))))\n",
    "                  )\n",
    "        AttributesRDDs.append(AttriRDD)\n",
    "        AttriClassRDD=(trainSet\n",
    "                        .map(lambda lp:((lp.label,lp.features[i]),1))\n",
    "                        .reduceByKey(lambda x,y:x+y)\n",
    "                        .map(lambda (x,y) : (x, y/float(RDDlen)))\n",
    "                        .map(lambda (x,y) : (x, np.log10(y/PClassesWithoutlog[x[1]]))) \n",
    "                      )\n",
    "        AttributeClassesRDDs.append(AttriClassRDD)\n",
    "        \n",
    "    return [PClassesRDD, AttributesRDDs, AttributeClassesRDDs, nAttri]\n",
    "\n",
    "def predict(model, objectToPredict):\n",
    "    Pc = model[0]\n",
    "    AttributesRDDs = model[1]\n",
    "    AttributeClassesRDDs = model[2]\n",
    "    nAttri = model[3]\n",
    "    sumClass = -1\n",
    "    bestClass = 0\n",
    "    Pa = []\n",
    "    Pac = []\n",
    "    #Filtrar RDD de probabilidade para conter somente as que sao necessarias para o objeto a ser predito e entao criar um dict\n",
    "    for i in range(nAttri):\n",
    "        Pa.append(AttributesRDDs[i].filter(lambda x : x[0] in objectToPredict.features).collectAsMap())\n",
    "        Pac.append(AttributeClassesRDDs[i].filter(lambda x : x[0][1] in objectToPredict.features).collectAsMap())\n",
    "    \n",
    "    ### AJUDEM AQUI\n",
    "    #Pa e Pac sao lists de dict, como converter para somente 1 dict?\n",
    "    \n",
    "    for i in Pc:\n",
    "        soma = sum(Pac[(i, a)] + Pc[i] - Pa[a] for a in objectToPredict.features)\n",
    "        if(soma > sumClass):\n",
    "            sumClass = soma\n",
    "            bestClass = i\n",
    "    return bestClass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
